# describes the mcp servers to use
mcpServers:
  filesystem:
    type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - .
  playwright:
    type: stdio
    command: npx
    args:
      - -y
      - "@playwright/mcp@latest"

  # hf:
  #   type: stdio
  #   command: npx
  #   args:
  #     - -y
  #     - "@llmindset/mcp-hfspace"

# System prompt configuration - defines the agent's behavior and instructions
systemPrompt:
  contributors:
    - id: primary
      type: static
      priority: 0
      content: |
        You are a helpful AI assistant with access to tools.
        Use these tools when appropriate to answer user queries.
        You can use multiple tools in sequence to solve complex problems.
        After each tool result, determine if you need more information or can provide a final answer.
    - id: dateTime
      type: dynamic
      priority: 10
      source: dateTime
      enabled: true

# Optional greeting shown at chat start (UI can consume this)
greeting: "Hi! I’m Dexto — how can I help today?"

# # describes the llm configuration
llm:
  provider: openai
  model: gpt-5-mini
  apiKey: $OPENAI_API_KEY

# Storage configuration - uses a two-tier architecture: cache (fast, ephemeral) and database (persistent, reliable)
# Memory cache with file-based database (good for development with persistence)
storage:
  cache:
    type: in-memory
  database:
    type: sqlite
    # path: ./data/dexto.db

## To use Google Gemini, replace the LLM section with Google Gemini configuration below
## Similar for anthropic/groq/etc.
# llm:
#   provider: google
#   model: gemini-2.0-flash
#   apiKey: $GOOGLE_GENERATIVE_AI_API_KEY

telemetry:
  serviceName: dexto-default-agent
  enabled: true
  tracerName: dexto-tracer
  export:
    type: otlp
    endpoint: http://localhost:4318/v1/traces # Default to localhost if not set
